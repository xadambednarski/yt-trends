{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "import nltk\n",
    "from num2words import num2words\n",
    "from langdetect import detect, lang_detect_exception\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True, raise_on_error=True, download_dir=\"../models/nltk_data\")\n",
    "nltk.download('punkt_tab', quiet=True, raise_on_error=True, download_dir=\"../models/nltk_data\")\n",
    "nltk.data.path.append(\"../models/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, file):\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "def load_json(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_translation_model(from_lang, to_lang):\n",
    "    models_dir = \"../models\"\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    if not os.path.exists(f\"{models_dir}/translation\"):\n",
    "        os.makedirs(f\"{models_dir}/translation\")\n",
    "    if f\"Helsinki-NLP/opus-mt-{from_lang}-{to_lang}\" not in os.listdir(f\"{models_dir}/translation\"):\n",
    "        return pipeline(\"translation\", model=f\"Helsinki-NLP/opus-mt-{from_lang}-{to_lang}\")\n",
    "    else:\n",
    "        return pipeline(\n",
    "            \"translation\",\n",
    "            model=f\"{models_dir}/translation/Helsinki-NLP/opus-mt-{from_lang}-{to_lang}\",\n",
    "        )\n",
    "\n",
    "\n",
    "def save_translation_model(model, from_lang, to_lang):\n",
    "    models_dir = \"../models\"\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    if not os.path.exists(f\"{models_dir}/translation\"):\n",
    "        os.makedirs(f\"{models_dir}/translation\")\n",
    "    model.save_pretrained(f\"{models_dir}/translation/Helsinki-NLP/opus-mt-{from_lang}-{to_lang}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    pattern = r\"\"\"\n",
    "        <.*?> |                               # HTML tags\n",
    "        \\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b |  # Emails\n",
    "        (https?:\\/\\/|www\\.)\\S+ |              # URLs starting with http, https, or www\n",
    "        \\b\\S+\\.(com|org|net|ly|co|ly|pl|uk)\\b    # Specific domains\n",
    "    \"\"\"\n",
    "    text = re.sub(pattern, \"\", text, flags=re.VERBOSE)\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_lowercase(text: str):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def demojize(text: str) -> str:\n",
    "    return emoji.replace_emoji(text, \"\")\n",
    "\n",
    "\n",
    "def convert_number(text: str) -> str:\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            try:\n",
    "                words[words.index(word)] = num2words(word)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "def remove_whitespace(text: str) -> str:\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    return \" \".join(word for word in words if word not in stop_words)\n",
    "\n",
    "\n",
    "def translate_text(text: str, from_lang, to_lang) -> str:\n",
    "    if from_lang == to_lang:\n",
    "        return text\n",
    "    try:\n",
    "        model = get_translation_model(from_lang, to_lang)\n",
    "        translation = model(text)\n",
    "        save_translation_model(model, from_lang, to_lang)\n",
    "        return translation[0][\"translation_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Translation failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text: str, lang: str = None) -> str:\n",
    "\n",
    "    text = clean_text(text)\n",
    "\n",
    "    text = text_lowercase(text)\n",
    "\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    text = remove_whitespace(text)\n",
    "    \n",
    "    text = demojize(text)\n",
    "    \n",
    "    if lang:\n",
    "        text = convert_number(text)\n",
    "        \n",
    "        text = translate_text(text, lang, \"en\")\n",
    "\n",
    "        text = remove_stopwords(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Entertainment in us\n",
      "Num of channels:  545\n",
      "Processing Film & Animation in us\n",
      "Num of channels:  94\n",
      "Processing Howto & Style in us\n",
      "Num of channels:  94\n",
      "Processing People & Blogs in us\n",
      "Num of channels:  220\n",
      "Processing Entertainment in pl\n",
      "Num of channels:  423\n",
      "Processing Film & Animation in pl\n",
      "Num of channels:  62\n",
      "Processing Howto & Style in pl\n",
      "Num of channels:  98\n",
      "Processing People & Blogs in pl\n",
      "Num of channels:  263\n"
     ]
    }
   ],
   "source": [
    "entertainment_us = load_json(\"../data/videos/last/united-states/Entertainment.json\")\n",
    "entertainment_pl = load_json(\"../data/videos/last/poland/Entertainment.json\")\n",
    "film_animation_us = load_json(\"../data/videos/last/united-states/Film & Animation.json\")\n",
    "film_animation_pl = load_json(\"../data/videos/last/poland/Film & Animation.json\")\n",
    "howto_style_us = load_json(\"../data/videos/last/united-states/Howto & Style.json\")\n",
    "howto_style_pl = load_json(\"../data/videos/last/poland/Howto & Style.json\")\n",
    "people_blogs_us = load_json(\"../data/videos/last/united-states/People & Blogs.json\")\n",
    "people_blogs_pl = load_json(\"../data/videos/last/poland/People & Blogs.json\")\n",
    "\n",
    "categories = [\"Entertainment\", \"Film & Animation\", \"Howto & Style\", \"People & Blogs\"]\n",
    "countries = [\"us\", \"pl\"]\n",
    "\n",
    "for country in countries:\n",
    "    for category in categories:\n",
    "        print(f\"Processing {category} in {country}\")\n",
    "        print(\"Num of channels: \", len(eval(f\"{category.lower().replace(\" & \", \"_\")}_{country}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(film_animation_pl, total=len(film_animation_pl), desc=\"Preprocessing film_animation_pl\"):\n",
    "    print(f\"Processing channel {channel['channel']['name']}\")\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"pl\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    film_animation_pl[film_animation_pl.index(channel)] = channel\n",
    "save_json(film_animation_pl, \"../data/videos/last/poland/Film & Animation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(film_animation_us, total=len(film_animation_us), desc=\"Preprocessing film_animation_us\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"en\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    film_animation_us[film_animation_us.index(channel)] = channel\n",
    "save_json(film_animation_us, \"../data/videos/last/united-states/Film & Animation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(entertainment_pl, total=len(entertainment_pl), desc=\"Preprocessing entertainment_pl\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"pl\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    entertainment_pl[entertainment_pl.index(channel)] = channel\n",
    "save_json(entertainment_pl, \"../data/videos/last/poland/Entertainment.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(entertainment_us, total=len(entertainment_us), desc=\"Preprocessing entertainment_us\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"en\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    entertainment_us[entertainment_us.index(channel)] = channel\n",
    "save_json(entertainment_us, \"../data/videos/last/united-states/Entertainment.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(howto_style_pl, total=len(howto_style_pl), desc=\"Preprocessing howto_style_pl\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"pl\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    howto_style_pl[howto_style_pl.index(channel)] = channel\n",
    "save_json(howto_style_pl, \"../data/videos/last/poland/Howto & Style.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(howto_style_us, total=len(howto_style_us), desc=\"Preprocessing howto_style_us\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"en\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    howto_style_us[howto_style_us.index(channel)] = channel\n",
    "save_json(howto_style_us, \"../data/videos/last/united-states/Howto & Style.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(people_blogs_pl, total=len(people_blogs_pl), desc=\"Preprocessing people_blogs_pl\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"pl\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    people_blogs_pl[people_blogs_pl.index(channel)] = channel\n",
    "save_json(people_blogs_pl, \"../data/videos/last/poland/People & Blogs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel in tqdm(people_blogs_us, total=len(people_blogs_us), desc=\"Preprocessing people_blogs_us\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"en\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    people_blogs_us[people_blogs_us.index(channel)] = channel\n",
    "save_json(people_blogs_us, \"../data/videos/last/united-states/People & Blogs.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
