{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "import nltk\n",
    "from num2words import num2words\n",
    "from langdetect import detect, lang_detect_exception\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download(\"stopwords\", quiet=True, raise_on_error=True, download_dir=\"../models/nltk_data\")\n",
    "nltk.download('punkt_tab', quiet=True, raise_on_error=True, download_dir=\"../models/nltk_data\")\n",
    "nltk.data.path.append(\"../models/nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'd\": \"who would\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why'd\": \"why did\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, file):\n",
    "    with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "def load_json(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def get_translation_model(from_lang, to_lang):\n",
    "    models_dir = \"../models\"\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    if not os.path.exists(f\"{models_dir}/translation\"):\n",
    "        os.makedirs(f\"{models_dir}/translation\")\n",
    "    if f\"Helsinki-NLP/opus-mt-{from_lang}-{to_lang}\" not in os.listdir(f\"{models_dir}/translation\"):\n",
    "        return pipeline(\"translation\", model=f\"Helsinki-NLP/opus-mt-{from_lang}-{to_lang}\")\n",
    "    else:\n",
    "        return pipeline(\n",
    "            \"translation\",\n",
    "            model=f\"{models_dir}/translation/Helsinki-NLP/opus-mt-{from_lang}-{to_lang}\",\n",
    "        )\n",
    "\n",
    "\n",
    "def save_translation_model(model, from_lang, to_lang):\n",
    "    models_dir = \"../models\"\n",
    "    if not os.path.exists(models_dir):\n",
    "        os.makedirs(models_dir)\n",
    "    if not os.path.exists(f\"{models_dir}/translation\"):\n",
    "        os.makedirs(f\"{models_dir}/translation\")\n",
    "    model.save_pretrained(f\"{models_dir}/translation/Helsinki-NLP/opus-mt-{from_lang}-{to_lang}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    pattern = r\"\"\"\n",
    "        <.*?> |                               # HTML tags\n",
    "        \\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b |  # Emails\n",
    "        (https?:\\/\\/|www\\.)\\S+ |              # URLs starting with http, https, or www\n",
    "        \\b\\S+\\.(com|org|net|ly|co|ly|pl|uk)\\b    # Specific domains\n",
    "    \"\"\"\n",
    "    text = re.sub(pattern, \"\", text, flags=re.VERBOSE)\n",
    "    # Remove non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]+\", \" \", text)\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    # 'nt -> not and 're -> are and so on   \n",
    "    text = ' '.join([contractions_dict.get(word, word) for word in text.split()])\n",
    "    # remove short words\n",
    "    text = ' '.join([word for word in text.split() if len(word) > 1])\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_lowercase(text: str):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def demojize(text: str) -> str:\n",
    "    return emoji.replace_emoji(text, \"\")\n",
    "\n",
    "\n",
    "def convert_number(text: str) -> str:\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            try:\n",
    "                words[words.index(word)] = num2words(word)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    return \" \".join(word for word in words if word not in stop_words)\n",
    "\n",
    "\n",
    "def translate_text(text: str, from_lang, to_lang) -> str:\n",
    "    if from_lang == to_lang:\n",
    "        return text\n",
    "    try:\n",
    "        model = get_translation_model(from_lang, to_lang)\n",
    "        translation = model(text)\n",
    "        save_translation_model(model, from_lang, to_lang)\n",
    "        return translation[0][\"translation_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Translation failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def preprocess_text(text: str, lang: str = None) -> str:\n",
    "\n",
    "    text = clean_text(text)\n",
    "\n",
    "    text = text_lowercase(text)\n",
    "\n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    text = demojize(text)\n",
    "    \n",
    "    # if lang:\n",
    "    #     text = convert_number(text)\n",
    "        \n",
    "    #     text = translate_text(text, lang, \"en\")\n",
    "\n",
    "    #     text = remove_stopwords(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Entertainment in us\n",
      "Num of channels:  545\n",
      "Processing Film & Animation in us\n",
      "Num of channels:  94\n",
      "Processing Howto & Style in us\n",
      "Num of channels:  94\n",
      "Processing People & Blogs in us\n",
      "Num of channels:  220\n",
      "Processing Entertainment in pl\n",
      "Num of channels:  423\n",
      "Processing Film & Animation in pl\n",
      "Num of channels:  62\n",
      "Processing Howto & Style in pl\n",
      "Num of channels:  98\n",
      "Processing People & Blogs in pl\n",
      "Num of channels:  263\n"
     ]
    }
   ],
   "source": [
    "entertainment_us = load_json(\"../data/videos/last/united-states/Entertainment.json\")\n",
    "entertainment_pl = load_json(\"../data/videos/last/poland/Entertainment.json\")\n",
    "film_animation_us = load_json(\"../data/videos/last/united-states/Film & Animation.json\")\n",
    "film_animation_pl = load_json(\"../data/videos/last/poland/Film & Animation.json\")\n",
    "howto_style_us = load_json(\"../data/videos/last/united-states/Howto & Style.json\")\n",
    "howto_style_pl = load_json(\"../data/videos/last/poland/Howto & Style.json\")\n",
    "people_blogs_us = load_json(\"../data/videos/last/united-states/People & Blogs.json\")\n",
    "people_blogs_pl = load_json(\"../data/videos/last/poland/People & Blogs.json\")\n",
    "\n",
    "categories = [\"Entertainment\", \"Film & Animation\", \"Howto & Style\", \"People & Blogs\"]\n",
    "countries = [\"us\", \"pl\"]\n",
    "\n",
    "for country in countries:\n",
    "    for category in categories:\n",
    "        print(f\"Processing {category} in {country}\")\n",
    "        print(\"Num of channels: \", len(eval(f\"{category.lower().replace(\" & \", \"_\")}_{country}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549f642cab064fc180468aa3e1c8e50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing film_animation_pl:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel in tqdm(film_animation_pl, total=len(film_animation_pl), desc=\"Preprocessing film_animation_pl\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"pl\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    film_animation_pl[film_animation_pl.index(channel)] = channel\n",
    "save_json(film_animation_pl, \"../data/videos/last/poland/Film & Animation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a1bc11c27046c59e3d7b4131f9810d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing film_animation_us:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel in tqdm(film_animation_us, total=len(film_animation_us), desc=\"Preprocessing film_animation_us\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"en\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    film_animation_us[film_animation_us.index(channel)] = channel\n",
    "save_json(film_animation_us, \"../data/videos/last/united-states/Film & Animation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296e02aec2e443e797e1272511d70936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing entertainment_pl:   0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel in tqdm(entertainment_pl, total=len(entertainment_pl), desc=\"Preprocessing entertainment_pl\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"pl\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    entertainment_pl[entertainment_pl.index(channel)] = channel\n",
    "save_json(entertainment_pl, \"../data/videos/last/poland/Entertainment.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01266dac7b84047bbc8b21cc993c642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing entertainment_us:   0%|          | 0/545 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel in tqdm(entertainment_us, total=len(entertainment_us), desc=\"Preprocessing entertainment_us\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"en\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    entertainment_us[entertainment_us.index(channel)] = channel\n",
    "save_json(entertainment_us, \"../data/videos/last/united-states/Entertainment.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50451d5513104adc99494abf2a33eb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing howto_style_pl:   0%|          | 0/98 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel in tqdm(howto_style_pl, total=len(howto_style_pl), desc=\"Preprocessing howto_style_pl\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"pl\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            video[\"title\"] = clean_title\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    howto_style_pl[howto_style_pl.index(channel)] = channel\n",
    "save_json(howto_style_pl, \"../data/videos/last/poland/Howto & Style.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11297aa8943240a68aa03e60611ae2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing howto_style_us:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel in tqdm(howto_style_us, total=len(howto_style_us), desc=\"Preprocessing howto_style_us\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"en\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    howto_style_us[howto_style_us.index(channel)] = channel\n",
    "save_json(howto_style_us, \"../data/videos/last/united-states/Howto & Style.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833040283a2948cb865ba2e6e026ac71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing people_blogs_pl:   0%|          | 0/263 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel in tqdm(people_blogs_pl, total=len(people_blogs_pl), desc=\"Preprocessing people_blogs_pl\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"pl\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    people_blogs_pl[people_blogs_pl.index(channel)] = channel\n",
    "save_json(people_blogs_pl, \"../data/videos/last/poland/People & Blogs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df6815e30094b1a83bcfdb451841f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing people_blogs_us:   0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for channel in tqdm(people_blogs_us, total=len(people_blogs_us), desc=\"Preprocessing people_blogs_us\"):\n",
    "    for video in channel[\"videos\"]:\n",
    "        try:\n",
    "            clean_title = preprocess_text(video[\"title\"], \"en\")\n",
    "            video[\"title\"] = clean_title\n",
    "        except Exception as e:\n",
    "            print(f\"Error for channel {channel['channel']['name']}: {e}\")\n",
    "    people_blogs_us[people_blogs_us.index(channel)] = channel\n",
    "save_json(people_blogs_us, \"../data/videos/last/united-states/People & Blogs.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
