{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poland: ['1 - 50.json', '101 - 150.json', '151 - 200.json', '201 - 250.json', '251 - 300.json', '301 - 350.json', '351 - 400.json', '51 - 100.json', 'progress.json']\n",
      "United States: []\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    poland_videos_files = sorted(os.listdir(\"../data/videos/last/poland\"))\n",
    "    print(f\"Poland: {poland_videos_files}\") \n",
    "    us_videos_files = sorted(os.listdir(\"../data/videos/last/united-states\"))\n",
    "    print(f\"United States: {us_videos_files}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_all = []\n",
    "us_all = []\n",
    "\n",
    "for file in poland_videos_files:\n",
    "    if file != \"progress.json\":\n",
    "        with open(f\"../data/videos/last/poland/{file}\", \"r\") as f:\n",
    "            videos = json.load(f)\n",
    "            pl_all.extend(videos)\n",
    "        \n",
    "for file in us_videos_files:\n",
    "    if file != \"progress.json\":\n",
    "        with open(f\"../data/videos/last/united-states/{file}\", \"r\") as f:\n",
    "            videos = json.load(f)\n",
    "            us_all.extend(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# sanity check\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pl_all) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(us_all) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m      5\u001b[0m unique_pl \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mint\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "assert len(pl_all) == 1000\n",
    "assert len(us_all) == 1000\n",
    "\n",
    "unique_pl = defaultdict(int)\n",
    "unique_us = defaultdict(int)\n",
    "\n",
    "for channel in pl_all:\n",
    "    unique_pl[channel[\"channel_id\"]] += 1\n",
    "\n",
    "for channel in us_all:\n",
    "    unique_us[channel[\"channel_id\"]] += 1\n",
    "    \n",
    "assert len(unique_pl) == 1000\n",
    "assert len(unique_us) == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_categories = {\n",
    "    \"1\": \"Film & Animation\",\n",
    "    \"2\": \"Autos & Vehicles\",\n",
    "    \"10\": \"Music\",\n",
    "    \"15\": \"Pets & Animals\",\n",
    "    \"17\": \"Sports\",\n",
    "    \"18\": \"Short Movies\",\n",
    "    \"19\": \"Travel & Events\",\n",
    "    \"20\": \"Gaming\",\n",
    "    \"21\": \"Videoblogging\",\n",
    "    \"22\": \"People & Blogs\",\n",
    "    \"23\": \"Comedy\",\n",
    "    \"24\": \"Entertainment\",\n",
    "    \"25\": \"News & Politics\",\n",
    "    \"26\": \"Howto & Style\",\n",
    "    \"27\": \"Education\",\n",
    "    \"28\": \"Science & Technology\",\n",
    "    \"29\": \"Nonprofits & Activism\",\n",
    "    \"30\": \"Movies\",\n",
    "    \"31\": \"Anime/Animation\",\n",
    "    \"32\": \"Action/Adventure\",\n",
    "    \"33\": \"Classics\",\n",
    "    \"34\": \"Comedy\",\n",
    "    \"35\": \"Documentary\",\n",
    "    \"36\": \"Drama\",\n",
    "    \"37\": \"Family\",\n",
    "    \"38\": \"Foreign\",\n",
    "    \"39\": \"Horror\",\n",
    "    \"40\": \"Sci-Fi/Fantasy\",\n",
    "    \"41\": \"Thriller\",\n",
    "    \"42\": \"Shorts\",\n",
    "    \"43\": \"Shows\",\n",
    "    \"44\": \"Trailers\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel TheNitroZyniak has no videos - setting category to Unknown\n",
      "Channel MagdalenaMariaMonika has no videos - setting category to Unknown\n",
      "Channel ZDROWE i SMACZNE przepisy. has no videos - setting category to Unknown\n",
      "Channel skkf has no videos - setting category to Unknown\n"
     ]
    }
   ],
   "source": [
    "for channel_idx in range(len(pl_all)):\n",
    "    try:\n",
    "        video_categories_count = defaultdict(int)\n",
    "        for video in pl_all[channel_idx][\"videos\"]:\n",
    "            video_categories_count[yt_categories[video[\"category\"]]] += 1\n",
    "        pl_all[channel_idx][\"channel\"][\"category\"] = max(video_categories_count, key=video_categories_count.get)\n",
    "    except ValueError:\n",
    "        print(f\"Channel {pl_all[channel_idx]['channel']['name']} has no videos - setting category to Unknown\")\n",
    "        pl_all[channel_idx][\"channel\"][\"category\"] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel_idx in range(len(us_all)):\n",
    "    try:\n",
    "        video_categories_count = defaultdict(int)\n",
    "        for video in us_all[channel_idx][\"videos\"]:\n",
    "            video_categories_count[yt_categories[video[\"category\"]]] += 1\n",
    "        us_all[channel_idx][\"channel\"][\"category\"] = max(video_categories_count, key=video_categories_count.get)\n",
    "    except ValueError:\n",
    "        print(f\"Channel {us_all[channel_idx]['channel']['name']} has no videos - setting category to Unknown\")\n",
    "        us_all[channel_idx][\"channel\"][\"category\"] = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "from num2words import num2words\n",
    "from langdetect import detect\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', '', text)\n",
    "    return text\n",
    "\n",
    "def text_lowercase(text: str):\n",
    "    return text.lower()\n",
    "\n",
    "def demojize(text: str):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "def convert_number(text: str) -> str:\n",
    "    words = text.split()\n",
    "    return ' '.join([num2words(word) if word.isdigit() else word for word in words])\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_whitespace(text: str) -> str:\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def remove_stopwords(text: str, language: str = 'english') -> str:\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    words = word_tokenize(text)\n",
    "    return ' '.join(word for word in words if word not in stop_words)\n",
    "\n",
    "def translate_text(text: str, target_language: str = 'en') -> str:\n",
    "    translator = pipeline(\"translation\", model=f\"Helsinki-NLP/opus-mt-{detect(text)}-{target_language}\")\n",
    "    return translator(text)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, target_language: str = 'en') -> str:\n",
    "    text = clean_text(text)\n",
    "\n",
    "    text = text_lowercase(text)\n",
    "\n",
    "    text = demojize(text)\n",
    "\n",
    "    text = convert_number(text)\n",
    "\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    text = remove_whitespace(text)\n",
    "\n",
    "    text = translate_text(text, target_language)\n",
    "\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        text = remove_stopwords(text, language=lang)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
